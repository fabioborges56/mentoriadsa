{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "first-breeding",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import requests\n",
    "\n",
    "from time import sleep\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "correct-vision",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\users\\asepulveda\\appdata\\local\\programs\\python\\python39\\lib\\genericpath.py\u001b[0m in \u001b[0;36mexists\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mOSError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] O sistema não pode encontrar o arquivo especificado: 'C:\\\\Users\\\\asepulveda/.netrc'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\ASEPUL~1\\AppData\\Local\\Temp/ipykernel_22380/2402822860.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     67\u001b[0m             url_lst.append(extract_text(str(element.find(\"h3\", class_=\"title\").find(\"a\", class_=\"PreviewTooltip\")), \n\u001b[0;32m     68\u001b[0m                                        'href=\\\"(.+?)\\\"\\ title'))\n\u001b[1;32m---> 69\u001b[1;33m             post_lst.append(extract_post(extract_text(str(element.find(\"h3\", class_=\"title\").find(\"a\", class_=\"PreviewTooltip\")), \n\u001b[0m\u001b[0;32m     70\u001b[0m                                        'href=\\\"(.+?)\\\"\\ title')))\n\u001b[0;32m     71\u001b[0m             \u001b[0marea_lst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marea\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\ASEPUL~1\\AppData\\Local\\Temp/ipykernel_22380/2402822860.py\u001b[0m in \u001b[0;36mextract_post\u001b[1;34m(link)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mextract_post\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlink\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0murl\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[1;34m\"https://forumjuridico.org/\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mlink\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mpage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"User-Agent\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\"XY\"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"html.parser\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"blockquote\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"messageText ugc baseHtml\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\asepulveda\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'allow_redirects'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'get'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\asepulveda\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\asepulveda\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    526\u001b[0m             \u001b[0mhooks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhooks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    527\u001b[0m         )\n\u001b[1;32m--> 528\u001b[1;33m         \u001b[0mprep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare_request\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    529\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m         \u001b[0mproxies\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mproxies\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\asepulveda\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mprepare_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    451\u001b[0m         \u001b[0mauth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauth\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    452\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrust_env\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mauth\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauth\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 453\u001b[1;33m             \u001b[0mauth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_netrc_auth\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    454\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    455\u001b[0m         \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPreparedRequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\asepulveda\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\requests\\utils.py\u001b[0m in \u001b[0;36mget_netrc_auth\u001b[1;34m(url, raise_errors)\u001b[0m\n\u001b[0;32m    190\u001b[0m                 \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 192\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    193\u001b[0m                 \u001b[0mnetrc_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\asepulveda\\appdata\\local\\programs\\python\\python39\\lib\\genericpath.py\u001b[0m in \u001b[0;36mexists\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;34m\"\"\"Test whether a path exists.  Returns False for broken symbolic links\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mOSError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Extraindo dados do primeiro forum escolhido\n",
    "\n",
    "def extract_text(text, pattern):\n",
    "    try:\n",
    "        found = re.search(pattern, text).group(1)\n",
    "    except AttributeError:\n",
    "        found = '0'\n",
    "    return found\n",
    "\n",
    "def extract_post(link):\n",
    "    url  = \"https://forumjuridico.org/\" + link\n",
    "    page = requests.get(url, headers={\"User-Agent\": \"XY\"})\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    results = soup.find(\"blockquote\", class_=\"messageText ugc baseHtml\").text\n",
    "    #sleep(1.0)\n",
    "    return results\n",
    "\n",
    "# Bloco onde definiremos a quantidade de paginas que vamos buscar dentro do fórum (cada pagina possui 20 perguntas)\n",
    "# e extraímos as informacoes mais basicas de cada uma (titulo, data, # de visitas, # de respostas e link para a postagem).\n",
    "\n",
    "areas = ['direito-do-trabalho.17', 'direito-de-familia.11', 'direito-administrativo.15',\n",
    "        'direito-civil-empresarial-e-do-consumidor.10', 'direito-penal-e-processo-penal.12']\n",
    "\n",
    "title_lst = []\n",
    "date_lst = []\n",
    "answer_lst = []\n",
    "visit_lst = []\n",
    "url_lst = []\n",
    "post_lst = []\n",
    "area_lst = []\n",
    "page_old = None\n",
    "\n",
    "for area in areas:\n",
    "    \n",
    "    for i in range(0, 100):\n",
    "        \n",
    "        url  = \"https://forumjuridico.org/forums/\" + area +\"/page-\" + str(i) + \"?_params=Array\"\n",
    "        page = requests.get(url, headers={\"User-Agent\": \"XY\"})\n",
    "             \n",
    "        # Sair do loop caso cheguemos na ultima pagina de uma determinada area no forum.\n",
    "        # Podemos fazer isso pois ao chegar na ultima pagina, se aumentaros o valor de i \n",
    "        # a ultima pagina continuará sendo mostrada\n",
    "        \n",
    "        if page == page_old:\n",
    "            break\n",
    "        \n",
    "        # Request para extrair o documento html\n",
    "        \n",
    "        soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "        results = soup.find(id=\"content\")\n",
    "        elements = soup.find_all(\"li\", class_=\"discussionListItem visible\")\n",
    "        page_old = page\n",
    "        \n",
    "        # Extração das informações pertinentes dentro de cada pagina\n",
    "\n",
    "        for element in elements:\n",
    "            title_lst.append(element.find(\"h3\", class_=\"title\").text.strip())\n",
    "            try:\n",
    "                date_lst.append(element.find(\"span\", class_=\"DateTime\").text.strip())\n",
    "            except AttributeError:\n",
    "                # A pagina de direito de familia utiliza um atributo diferente para o div, entao tratamos o erro aqui\n",
    "                # para buscar o atributo correto\n",
    "                date_lst.append(element.find(\"abbr\", class_=\"DateTime\").text.strip())\n",
    "                \n",
    "            answer_lst.append(extract_text(element.find(\"dl\", class_=\"major\").text.strip(), '.+?(\\d)'))\n",
    "            visit_lst.append(extract_text(element.find(\"dl\", class_=\"minor\").text.strip(), '.+?(\\d)'))\n",
    "            url_lst.append(extract_text(str(element.find(\"h3\", class_=\"title\").find(\"a\", class_=\"PreviewTooltip\")), \n",
    "                                       'href=\\\"(.+?)\\\"\\ title'))\n",
    "            post_lst.append(extract_post(extract_text(str(element.find(\"h3\", class_=\"title\").find(\"a\", class_=\"PreviewTooltip\")), \n",
    "                                       'href=\\\"(.+?)\\\"\\ title')))\n",
    "            area_lst.append(area)\n",
    "\n",
    "    # Caso seja a primeira area, criar o data frame. Caso contrario, realizar o append\n",
    "    \n",
    "    if area == 'direito-do-trabalho.17':\n",
    "        posts_df = pd.DataFrame(\n",
    "            {'title': title_lst,\n",
    "             'date': date_lst,\n",
    "             'answers': answer_lst,\n",
    "             'visits': visit_lst,\n",
    "             'url': url_lst,\n",
    "             'post': post_lst,\n",
    "             'area': area_lst\n",
    "            })\n",
    "    else:\n",
    "        posts_df = posts_df.append(\n",
    "            pd.DataFrame(\n",
    "            {'title': title_lst,\n",
    "             'date': date_lst,\n",
    "             'answers': answer_lst,\n",
    "             'visits': visit_lst,\n",
    "             'url': url_lst,\n",
    "             'post': post_lst,\n",
    "             'area': area_lst\n",
    "            })\n",
    "        )\n",
    "\n",
    "posts_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oriented-filling",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraindo dados de direito empresarial\n",
    "\n",
    "def extract_text(text, pattern):\n",
    "    try:\n",
    "        found = re.search(pattern, text).group(1)\n",
    "    except AttributeError:\n",
    "        found = 0\n",
    "    return found\n",
    "\n",
    "def extract_post(link):\n",
    "    url  = \"https://www.perguntedireito.com.br\" + link\n",
    "    page = requests.get(url, headers={\"User-Agent\": \"XY\"})\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    results = extract_text(str(soup), 'itemprop\\=\\\"text\\\"\\>(.+?)\\<\\/div')\n",
    "    #sleep(1.0)\n",
    "    return results\n",
    "\n",
    "# Bloco onde definiremos a quantidade de paginas que vamos buscar dentro do fórum (cada pagina possui 10 perguntas)\n",
    "# e extraímos as informacoes mais basicas de cada uma (titulo, data, # de visitas, # de respostas e link para a postagem).\n",
    "\n",
    "for i in range(0, 20):\n",
    "\n",
    "    url  = \"https://www.perguntedireito.com.br/questions/direito-empresarial?start=\" + str(i * 10)\n",
    "    page = requests.get(url, headers={\"User-Agent\": \"XY\"})\n",
    "\n",
    "    # Request para extrair o documento html\n",
    "\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    elements = soup.find_all(\"div\", class_=\"qa-q-list-item\")\n",
    "\n",
    "    title_lst = []\n",
    "    date_lst = []\n",
    "    answer_lst = []\n",
    "    url_lst = []\n",
    "    post_lst = []\n",
    "\n",
    "    for element in elements:\n",
    "        title_lst.append(element.find(\"div\", class_=\"qa-q-item-title\").text.strip())\n",
    "        date_lst.append(element.find(\"span\", class_=\"qa-q-item-when-data\").text.strip())\n",
    "        answer_lst.append(extract_text(element.find(\"span\", class_=\"qa-a-count-data\").text.strip(), '.+?(\\d)'))\n",
    "        url_lst.append(extract_text(str(element.find(\"div\", class_=\"qa-q-item-title\")), \n",
    "                                   'href=\\\"(.+?)\\\"\\>'))\n",
    "        post_lst.append(\n",
    "            extract_post(\n",
    "                extract_text(\n",
    "                    str(\n",
    "                        element.find(\"div\", class_=\"qa-q-item-title\")\n",
    "                    ),'href=\\\"(.+?)\\\"\\>')\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Transformando as listas em um dataframe e visualizando o resultado\n",
    "\n",
    "    if i == 0:\n",
    "        empresarial_df = pd.DataFrame(\n",
    "            {'title': title_lst,\n",
    "             'date': date_lst,\n",
    "             'answers': answer_lst,\n",
    "             'url': url_lst,\n",
    "             'post': post_lst,\n",
    "             'area': 'empresarial'\n",
    "            })\n",
    "    else:\n",
    "        empresarial_df = empresarial_df.append(\n",
    "            pd.DataFrame(\n",
    "            {'title': title_lst,\n",
    "             'date': date_lst,\n",
    "             'answers': answer_lst,\n",
    "             'url': url_lst,\n",
    "             'post': post_lst,\n",
    "             'area': 'empresarial'\n",
    "            })\n",
    "        )\n",
    "\n",
    "empresarial_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "raising-programmer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvandos os dataframes em um arquivo csv\n",
    "\n",
    "posts_df.to_csv('dados\\posts.csv', sep='|', header=True, index=False)\n",
    "empresarial_df.to_csv('dados\\posts_2.csv', sep='|', header=True, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
