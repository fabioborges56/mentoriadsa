{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd\r\n",
    "import re\r\n",
    "import requests\r\n",
    "\r\n",
    "from time import sleep\r\n",
    "from bs4 import BeautifulSoup"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Extraindo dados do primeiro forum escolhido\r\n",
    "\r\n",
    "def extract_text(text, pattern):\r\n",
    "    try:\r\n",
    "        found = re.search(pattern, text).group(1)\r\n",
    "    except AttributeError:\r\n",
    "        found = '0'\r\n",
    "    return found\r\n",
    "\r\n",
    "def extract_post(link):\r\n",
    "    url  = \"https://forumjuridico.org/\" + link\r\n",
    "    page = requests.get(url, headers={\"User-Agent\": \"XY\"})\r\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\r\n",
    "    results = soup.find(\"blockquote\", class_=\"messageText ugc baseHtml\").text\r\n",
    "    #sleep(1.0)\r\n",
    "    return results\r\n",
    "\r\n",
    "# Bloco onde definiremos a quantidade de paginas que vamos buscar dentro do fórum (cada pagina possui 20 perguntas)\r\n",
    "# e extraímos as informacoes mais basicas de cada uma (titulo, data, # de visitas, # de respostas e link para a postagem).\r\n",
    "\r\n",
    "areas = ['direito-do-trabalho.17', 'direito-de-familia.11', 'direito-administrativo.15',\r\n",
    "        'direito-civil-empresarial-e-do-consumidor.10', 'direito-penal-e-processo-penal.12']\r\n",
    "\r\n",
    "title_lst = []\r\n",
    "date_lst = []\r\n",
    "answer_lst = []\r\n",
    "visit_lst = []\r\n",
    "url_lst = []\r\n",
    "post_lst = []\r\n",
    "area_lst = []\r\n",
    "page_old = None\r\n",
    "\r\n",
    "for area in areas:\r\n",
    "    \r\n",
    "    for i in range(0, 100):\r\n",
    "        \r\n",
    "        url  = \"https://forumjuridico.org/forums/\" + area +\"/page-\" + str(i) + \"?_params=Array\"\r\n",
    "        page = requests.get(url, headers={\"User-Agent\": \"XY\"})\r\n",
    "             \r\n",
    "        # Sair do loop caso cheguemos na ultima pagina de uma determinada area no forum.\r\n",
    "        # Podemos fazer isso pois ao chegar na ultima pagina, se aumentaros o valor de i \r\n",
    "        # a ultima pagina continuará sendo mostrada\r\n",
    "        \r\n",
    "        if page == page_old:\r\n",
    "            break\r\n",
    "        \r\n",
    "        # Request para extrair o documento html\r\n",
    "        \r\n",
    "        soup = BeautifulSoup(page.content, \"html.parser\")\r\n",
    "        results = soup.find(id=\"content\")\r\n",
    "        elements = soup.find_all(\"li\", class_=\"discussionListItem visible\")\r\n",
    "        page_old = page\r\n",
    "        \r\n",
    "        # Extração das informações pertinentes dentro de cada pagina\r\n",
    "\r\n",
    "        for element in elements:\r\n",
    "            title_lst.append(element.find(\"h3\", class_=\"title\").text.strip())\r\n",
    "            try:\r\n",
    "                date_lst.append(element.find(\"span\", class_=\"DateTime\").text.strip())\r\n",
    "            except AttributeError:\r\n",
    "                # A pagina de direito de familia utiliza um atributo diferente para o div, entao tratamos o erro aqui\r\n",
    "                # para buscar o atributo correto\r\n",
    "                date_lst.append(element.find(\"abbr\", class_=\"DateTime\").text.strip())\r\n",
    "                \r\n",
    "            answer_lst.append(extract_text(element.find(\"dl\", class_=\"major\").text.strip(), '.+?(\\d)'))\r\n",
    "            visit_lst.append(extract_text(element.find(\"dl\", class_=\"minor\").text.strip(), '.+?(\\d)'))\r\n",
    "            url_lst.append(extract_text(str(element.find(\"h3\", class_=\"title\").find(\"a\", class_=\"PreviewTooltip\")), \r\n",
    "                                       'href=\\\"(.+?)\\\"\\ title'))\r\n",
    "            post_lst.append(extract_post(extract_text(str(element.find(\"h3\", class_=\"title\").find(\"a\", class_=\"PreviewTooltip\")), \r\n",
    "                                       'href=\\\"(.+?)\\\"\\ title')))\r\n",
    "            area_lst.append(area)\r\n",
    "\r\n",
    "    # Caso seja a primeira area, criar o data frame. Caso contrario, realizar o append\r\n",
    "    \r\n",
    "    if area == 'direito-do-trabalho.17':\r\n",
    "        posts_df = pd.DataFrame(\r\n",
    "            {'title': title_lst,\r\n",
    "             'date': date_lst,\r\n",
    "             'answers': answer_lst,\r\n",
    "             'visits': visit_lst,\r\n",
    "             'url': url_lst,\r\n",
    "             'post': post_lst,\r\n",
    "             'area': area_lst\r\n",
    "            })\r\n",
    "    else:\r\n",
    "        posts_df = posts_df.append(\r\n",
    "            pd.DataFrame(\r\n",
    "            {'title': title_lst,\r\n",
    "             'date': date_lst,\r\n",
    "             'answers': answer_lst,\r\n",
    "             'visits': visit_lst,\r\n",
    "             'url': url_lst,\r\n",
    "             'post': post_lst,\r\n",
    "             'area': area_lst\r\n",
    "            })\r\n",
    "        )\r\n",
    "\r\n",
    "posts_df.head()\r\n",
    "\r\n",
    "posts_df.to_csv('dados\\posts.csv', sep='|', header=True, index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Extraindo dados de do segundo forum escolhido\r\n",
    "\r\n",
    "def extract_text(text, pattern):\r\n",
    "    try:\r\n",
    "        found = re.search(pattern, text).group(1)\r\n",
    "    except AttributeError:\r\n",
    "        found = 0\r\n",
    "    return found\r\n",
    "\r\n",
    "def extract_post(link):\r\n",
    "    url  = \"https://www.perguntedireito.com.br\" + link\r\n",
    "    page = requests.get(url, headers={\"User-Agent\": \"XY\"})\r\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\r\n",
    "    results = extract_text(str(soup), 'itemprop\\=\\\"text\\\"\\>(.+?)\\<\\/div')\r\n",
    "    #sleep(1.0)\r\n",
    "    return results\r\n",
    "\r\n",
    "# Bloco onde definiremos a quantidade de paginas que vamos buscar dentro do fórum (cada pagina possui 10 perguntas)\r\n",
    "# e extraímos as informacoes mais basicas de cada uma (titulo, data, # de visitas, # de respostas e link para a postagem).\r\n",
    "\r\n",
    "for i in range(0, 20):\r\n",
    "\r\n",
    "    url  = \"https://www.perguntedireito.com.br/questions/direito-empresarial?start=\" + str(i * 10)\r\n",
    "    page = requests.get(url, headers={\"User-Agent\": \"XY\"})\r\n",
    "\r\n",
    "    # Request para extrair o documento html\r\n",
    "\r\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\r\n",
    "    elements = soup.find_all(\"div\", class_=\"qa-q-list-item\")\r\n",
    "\r\n",
    "    title_lst = []\r\n",
    "    date_lst = []\r\n",
    "    answer_lst = []\r\n",
    "    url_lst = []\r\n",
    "    post_lst = []\r\n",
    "\r\n",
    "    for element in elements:\r\n",
    "        title_lst.append(element.find(\"div\", class_=\"qa-q-item-title\").text.strip())\r\n",
    "        date_lst.append(element.find(\"span\", class_=\"qa-q-item-when-data\").text.strip())\r\n",
    "        answer_lst.append(extract_text(element.find(\"span\", class_=\"qa-a-count-data\").text.strip(), '.+?(\\d)'))\r\n",
    "        url_lst.append(extract_text(str(element.find(\"div\", class_=\"qa-q-item-title\")), \r\n",
    "                                   'href=\\\"(.+?)\\\"\\>'))\r\n",
    "        post_lst.append(\r\n",
    "            extract_post(\r\n",
    "                extract_text(\r\n",
    "                    str(\r\n",
    "                        element.find(\"div\", class_=\"qa-q-item-title\")\r\n",
    "                    ),'href=\\\"(.+?)\\\"\\>')\r\n",
    "            )\r\n",
    "        )\r\n",
    "\r\n",
    "    # Transformando as listas em um dataframe e visualizando o resultado\r\n",
    "\r\n",
    "    if i == 0:\r\n",
    "        empresarial_df = pd.DataFrame(\r\n",
    "            {'title': title_lst,\r\n",
    "             'date': date_lst,\r\n",
    "             'answers': answer_lst,\r\n",
    "             'url': url_lst,\r\n",
    "             'post': post_lst,\r\n",
    "             'area': 'empresarial'\r\n",
    "            })\r\n",
    "    else:\r\n",
    "        empresarial_df = empresarial_df.append(\r\n",
    "            pd.DataFrame(\r\n",
    "            {'title': title_lst,\r\n",
    "             'date': date_lst,\r\n",
    "             'answers': answer_lst,\r\n",
    "             'url': url_lst,\r\n",
    "             'post': post_lst,\r\n",
    "             'area': 'empresarial'\r\n",
    "            })\r\n",
    "        )\r\n",
    "\r\n",
    "empresarial_df.head()\r\n",
    "\r\n",
    "empresarial_df.to_csv('dados\\posts_2.csv', sep='|', header=True, index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Extraindo dados do terceiro forum escolhido\r\n",
    "\r\n",
    "def extract_text(text, pattern):\r\n",
    "    try:\r\n",
    "        found = re.search(pattern, text).group(1)\r\n",
    "    except AttributeError:\r\n",
    "        found = '0'\r\n",
    "    return found\r\n",
    "\r\n",
    "def extract_post(link):\r\n",
    "    page = requests.get(link, headers={\"User-Agent\": \"XY\"})\r\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\r\n",
    "    results = soup.find(\"div\", class_=\"question-body ask_content\").text\r\n",
    "    return results\r\n",
    "\r\n",
    "url_base = 'https://jus.com.br/duvidas/'\r\n",
    "areas = [\r\n",
    "    'direito-administrativo/',\r\n",
    "    'direito-do-trabalho/',\r\n",
    "    'direito-penal/',\r\n",
    "    'direito-processual-penal/',\r\n",
    "    'direito-civil/',\r\n",
    "    'direito-do-consumidor/',\r\n",
    "    'direito-de-familia/'\r\n",
    "]\r\n",
    "\r\n",
    "title_lst = []\r\n",
    "date_lst = []\r\n",
    "answer_lst = []\r\n",
    "like_lst = []\r\n",
    "url_lst = []\r\n",
    "post_lst = []\r\n",
    "area_lst = []\r\n",
    "page_old = None\r\n",
    "\r\n",
    "\r\n",
    "for area in areas:\r\n",
    "    \r\n",
    "    for i in range(0, 100):\r\n",
    "        \r\n",
    "        url  = url_base + area + '/p/' + str(i+1)\r\n",
    "        page = requests.get(url, headers={\"User-Agent\": \"XY\"})\r\n",
    "             \r\n",
    "        # Sair do loop caso cheguemos na ultima pagina de uma determinada area no forum.\r\n",
    "        # Podemos fazer isso pois ao chegar na ultima pagina, se aumentaros o valor de i \r\n",
    "        # a ultima pagina continuará sendo mostrada\r\n",
    "\r\n",
    "        if page == page_old:\r\n",
    "            break\r\n",
    "        \r\n",
    "        # Request para extrair o documento html\r\n",
    "        \r\n",
    "        soup = BeautifulSoup(page.content, \"html.parser\")\r\n",
    "        results = soup.find(id=\"questions\")\r\n",
    "        elements = results.find_all(\"li\", class_=\"\")\r\n",
    "        page_old = page\r\n",
    "        \r\n",
    "        # Extração das informações pertinentes dentro de cada pagina\r\n",
    "\r\n",
    "        for element in elements:\r\n",
    "\r\n",
    "            if len(str(element)) < 400:\r\n",
    "                continue\r\n",
    "\r\n",
    "            title_lst.append(element.find(\"a\", class_=\"title\").text.strip())\r\n",
    "            date_lst.append(element.find(\"abbr\", class_=\"timeago\").text.strip())\r\n",
    "            try:\r\n",
    "                answer_lst.append(element.find(\"span\", class_=\"answers-count striped\").text.strip())\r\n",
    "            except Exception as e:\r\n",
    "                answer_lst.append(\"0\")\r\n",
    "            like_lst.append(element.find(\"i\", class_=\"fa fa-heart\").text.strip())\r\n",
    "            # <span class=\"likes\"><i class=\"fa fa-heart\"></i> 0</span>\r\n",
    "            url_lst.append(extract_text(str(element.find(\"a\", class_=\"title\")), \r\n",
    "                                       'href=\\\"(.+?)\\\"\\>'))\r\n",
    "            post_lst.append(extract_post(extract_text(str(element.find(\"a\", class_=\"title\")), \r\n",
    "                                       'href=\\\"(.+?)\\\"\\>')))\r\n",
    "            area_lst.append(area)\r\n",
    "\r\n",
    "    # Caso seja a primeira area, criar o data frame. Caso contrario, realizar o append\r\n",
    "    \r\n",
    "    if area == 'direito-administrativo/':\r\n",
    "        posts_df = pd.DataFrame(\r\n",
    "            {'title': title_lst,\r\n",
    "             'date': date_lst,\r\n",
    "             'answers': answer_lst,\r\n",
    "             'likes': like_lst,\r\n",
    "             'url': url_lst,\r\n",
    "             'post': post_lst,\r\n",
    "             'area': area_lst\r\n",
    "            })\r\n",
    "    else:\r\n",
    "        posts_df = posts_df.append(\r\n",
    "            pd.DataFrame(\r\n",
    "            {'title': title_lst,\r\n",
    "             'date': date_lst,\r\n",
    "             'answers': answer_lst,\r\n",
    "             'likes': like_lst,\r\n",
    "             'url': url_lst,\r\n",
    "             'post': post_lst,\r\n",
    "             'area': area_lst\r\n",
    "            })\r\n",
    "        )\r\n",
    "\r\n",
    "\r\n",
    "posts_df.to_csv('dados\\posts_4.csv', sep='|', header=True, index=False)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "interpreter": {
   "hash": "f073831f69cd3c6d3e66726246945d904cffd54393bb30d63848c4ab17154cac"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}